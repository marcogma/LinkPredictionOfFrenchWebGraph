{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LinkPrediction (1).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GMix1j3-yOHy"},"source":["To run this notebook it is necessary to have installed the following packages: sklearn, networkx, node2vec, gensim and glob"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Aaq7LcjAmW8Q"},"source":["# Imports & Initialization"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yeoFVkyQl42v","colab":{}},"source":["import networkx as nx\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, accuracy_score, roc_curve, auc\n","from sklearn import preprocessing \n","from sklearn.preprocessing import MinMaxScaler\n","import csv\n","import collections\n","import matplotlib.pyplot as plt\n","import pandas as pd \n","from matplotlib import pylab as pl\n","from gensim.models import Word2Vec\n","from node2vec import Node2Vec\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import GridSearchCV\n","\n","import glob\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","path = \"./\"\n","pathTrainingData = path + \"training.txt\"\n","pathTestData = path+\"testing.txt\"\n","#pathPages =path+'node_information/text/'\n","#pathPages =path+'data/'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MaqJjFg4nUwI"},"source":["## GRAPH CREATION"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-c055pSgnhhD","colab":{}},"source":["G = nx.Graph() #undirected graph\n","GDi = nx.DiGraph() #directed graph\n","X=[]\n","Y=[]\n","X_Kaggle=[]\n","\n","with open(pathTrainingData, \"r\") as f:\n","    for line in f:\n","        line = line.split()\n","        X.append([line[0], line[1]])\n","        Y.append(int(line[2]))\n","        if line[2]=='1':\n","            G.add_edge(line[0], line[1])\n","            GDi.add_edge(line[0], line[1])\n","        else:\n","            G.add_nodes_from([line[0],line[1]])\n","            GDi.add_nodes_from([line[0], line[1]])\n","\n","with open(pathTestData, \"r\") as f:\n","    for line in f:\n","        line = line.split()\n","        X_Kaggle.append([line[0], line[1]])\n","\n","for n in G.nodes:\n","    G.nodes[n]['community'] = 0\n","\n","for n in GDi.nodes:\n","    GDi.nodes[n]['community'] = 0\n","\n","X=np.array(X)\n","X_Kaggle=np.array(X_Kaggle)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tMX-HgMSvQgD"},"source":["# Graph Information"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-HtZQuevvUr3","colab":{}},"source":["print(nx.info(GDi))\n","print('average clustering coefficient:  ' , nx.average_clustering(GDi))\n","#print('average shortest path length:  ' , nx.average_shortest_path_length(G))\n","degreesDi = [GDi.degree(n) for n in GDi.nodes()]\n","deg_hist=np.histogram(degreesDi, bins=[0,1, 2, 10, 20,30,40,50,60,70,80,90,100])\n","plt.hist(degreesDi, bins=[0,1, 2, 10, 20,30,40,50,60,70,80,90,100])\n","plt.title('Graph Degree')\n","plt.xlabel('Degree')\n","plt.ylabel('NÂ° vertices')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zfDILxM5pFs4"},"source":["# Graph Feature Extraction"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e4gpoSEynuyV","colab":{}},"source":["# Features extraction methods\n","\n","def jaccard(G,X):\n","    jaccardcoef=[]\n","    for i in range(X.shape[0]):\n","        try:\n","            coef = [[u, v, p]for u, v, p in nx.jaccard_coefficient(G, [(X[i][0], X[i][1])])][0]\n","            jaccardcoef.append(coef[2])\n","        except:\n","            jaccardcoef.append(0)\n","    return jaccardcoef\n","\n","def adamic(G,X):\n","    adamicix=[]\n","    for i in range(X.shape[0]):\n","        try:\n","            coef = [[u, v, p]for u, v, p in nx.adamic_adar_index(G, [(X[i][0], X[i][1])])][0]\n","            adamicix.append(coef[2])\n","        except:\n","            adamicix.append(0)\n","    return adamicix\n","\n","def preferentialAttachment(G,X):\n","    preferentialAtt=[]\n","    for i in range(X.shape[0]):\n","        try:\n","            coef = [[u, v, p]for u, v, p in nx.preferential_attachment(G, [(X[i][0], X[i][1])])][0]\n","            preferentialAtt.append(coef[2])\n","        except:\n","            preferentialAtt.append(0)\n","    return preferentialAtt\n","\n","def resourceAllocation(G,X):\n","    resource_allocation=[]\n","    for i in range(X.shape[0]):\n","        try:\n","            coef = [[u, v, p]for u, v, p in nx.resource_allocation_index(G, [(X[i][0], X[i][1])])][0]\n","            resource_allocation.append(coef[2])\n","        except:\n","            resource_allocation.append(0)\n","    return resource_allocation\n","\n","def soundarajan_hopcroft(G,X):\n","    soundarajan=[]\n","    for i in range(X.shape[0]):\n","        try:\n","            coef = [[u, v, p]for u, v, p in nx.cn_soundarajan_hopcroft(G, [(X[i][0], X[i][1])])][0]\n","            soundarajan.append(coef[2])\n","        except:\n","            soundarajan.append(0)\n","    return soundarajan\n","\n","def commonNeighbors(G,X):\n","    commonN=[]\n","    for i in range(X.shape[0]):\n","        try:\n","            shortestArr =  nx.common_neighbors(G, X[i,0], X[i,1])\n","            commonN.append(len(sorted(shortestArr)))\n","        except:\n","            commonN.append(0)\n","    return commonN"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"STswXKtytuMG"},"source":["# Graph Embeddings"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VSX3JVgit62I","colab":{}},"source":["def TrainGraphEmbeddings(GDi):\n","    #node2vec = Node2Vec(GDi, dimensions=16, walk_length=30, num_walks=200, workers=1)\n","    node2vec = Node2Vec(GDi, dimensions=4, walk_length=5, num_walks=5, workers=1)\n","    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n","    # Save model for later use\n","    #model.save(path+\"embeddingModeldim64Di.model\")\n","    return model\n","\n","def NodeSimilarity(NodePairs,model):\n","    node_sim=[]\n","    for nodepair in NodePairs:\n","        try:\n","            node_sim.append(model.wv.similarity(nodepair[0],nodepair[1]))\n","        except:\n","            node_sim.append(0.0)\n","    return node_sim"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mcl594Ep1Wux"},"source":["# Text Feature Extraction\n","Uncomment the codes of this section if the pathPages was specified and is correct"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TVE5TU62YMrS","colab":{}},"source":["#stop_words = open(path+'stopwords.txt','r').read().split(',') #Loading the prespecified stop words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vUmXIePiPP6Q","colab":{}},"source":["'''pages = []\n","for i in range(len(G.nodes)):\n","    f = open(pathPages+ str(i) + \".txt\", encoding=\"utf8\", errors='ignore')\n","    pages.append(re.sub('[0-9_]', '', f.read()))'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cxFWt_XgO_or","colab":{}},"source":["#vectorizer = TfidfVectorizer(analyzer='word', stop_words=stop_words, min_df=0.00005) #TF-IDF model "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Qeh57uYWPep5","colab":{}},"source":["#%time vectors_pages = vectorizer.fit_transform(pages) #Vectorizing pages"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VYPY8etDUQsB","colab":{}},"source":["#cos_sim_matrix = cosine_similarity(vectors_pages)#This operation may require a lot of RAM"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GfP7iNgJXcIZ","colab":{}},"source":["'''cos_sim = []\n","for e in X:\n","    idx1 = int(e[0])\n","    idx2 = int(e[1])\n","    cos_sim = cos_sim_matrix[idx1,idx2]\n","cos_sim = np.array(cos_sim)'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MaI5iueZaWBB","colab":{}},"source":["'''cos_sim_kaggle = []\n","for e in X_Kaggle:\n","    idx1 = int(e[0])\n","    idx2 = int(e[1])\n","    cos_sim_kaggle = cos_sim_matrix[idx1,idx2]\n","cos_sim_kaggle = np.array(cos_sim_kaggle)'''"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CWolOda3yBxH"},"source":["# Compute Features"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qtjz5iNlDuiL"},"source":["#### Training features"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JWVpwETGdQRP","colab":{}},"source":["cos_sim=[] #Loading of the precomputed cosine similarities of the training nodes pages(text)\n","with open(path+\"cos_sim.txt\", \"r\") as f:\n","    for line in f:\n","        line = line.split()\n","        cos_sim.append(float(line[0]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lSpCBE9kDUCM","colab":{}},"source":["M=X.shape[0]\n","N= 8 # 7 extracted features\n","G_Features= np.zeros((M,N))\n","\n","#model=TrainGraphEmbeddings(GDi)\n","model = Word2Vec.load(path+\"embeddingModeldim32.model\")\n","print('computing Jaccard...')\n","G_Features[:,0] = jaccard(G,X)\n","print('computing Adamic...')\n","G_Features[:,1] = adamic(G,X)\n","print('computing preferentialAtt...')\n","G_Features[:,2] = preferentialAttachment(G,X)\n","print('computing resource_allocation...')\n","G_Features[:,3] = resourceAllocation(G,X)\n","print('computing soundarajan...')\n","G_Features[:,4] = soundarajan_hopcroft(G,X)\n","print('computing commonNeighbors...')\n","G_Features[:,5] = commonNeighbors(G,X)\n","print('computing NodeSimilarity...')\n","G_Features[:,6] = NodeSimilarity(X, model)\n","print('computing cos_similarity_text...')\n","G_Features[:,7] = cos_sim\n","print('Finish')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R3AE6OPVxiUi","colab_type":"text"},"source":["#### Kaggle Test features"]},{"cell_type":"code","metadata":{"id":"3hYIKgBaxiUk","colab_type":"code","colab":{}},"source":["cos_sim_kaggle=[] #Loading of the precomputed cosine similarities of the test nodes pages(text)\n","with open(path+\"cos_sim_test.txt\", \"r\") as f:\n","    for line in f:\n","        line = line.split()\n","        cos_sim_kaggle.append(float(line[0]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"N3ESE4HHr_X9","colab":{}},"source":["MKaggle=X_Kaggle.shape[0]\n","GKaggle_Features= np.zeros((MKaggle,N))\n","#model=TrainGraphEmbeddings(GDi)\n","model = Word2Vec.load(path+\"embeddingModeldim32.model\")\n","print('computing Jaccard...')\n","GKaggle_Features[:,0] = jaccard(G,X_Kaggle)\n","print('computing Adamic...')\n","GKaggle_Features[:,1] = adamic(G,X_Kaggle)\n","print('computing preferentialAtt...')\n","GKaggle_Features[:,2] = preferentialAttachment(G,X_Kaggle)\n","print('computing resource_allocation...')\n","GKaggle_Features[:,3] = resourceAllocation(G,X_Kaggle)\n","print('computing soundarajan...')\n","GKaggle_Features[:,4] = soundarajan_hopcroft(G,X_Kaggle)\n","print('computing commonNeighbors...')\n","GKaggle_Features[:,5] = commonNeighbors(G,X_Kaggle)\n","print('computing NodeSimilarity...')\n","GKaggle_Features[:,6] = NodeSimilarity(X_Kaggle, model)\n","print('computing cos_similarity...')\n","GKaggle_Features[:,7] = cos_sim_kaggle\n","print('Finish')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iHDVSo1HAl7J"},"source":["#### Save Training Features"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KHNiCCFgAjup","colab":{}},"source":["pd.DataFrame(G_Features).to_csv(path+\"XFeatures.csv\", header=None, index=None)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"izd7VlD3EGYv"},"source":["#### Save Kaggle Features"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xfdLfw7JEKb4","colab":{}},"source":["pd.DataFrame(GKaggle_Features).to_csv(path+\"XKaggleFeatures.csv\", header=None, index=None)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"F360l3fGCOoL"},"source":["# Load saved features"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ay-0nPKpJG3i"},"source":["Load node2vec model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AFna4gydJGWt","colab":{}},"source":["model = Word2Vec.load(path+\"embeddingModeldim64Di.model\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HjVPLDmZEXZ3"},"source":["Load Training Features"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1GzpxAe89JCa","colab":{}},"source":["G_Features=np.genfromtxt(path+\"XFeatures.csv\", delimiter=',')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HbqoLJnVEaH0"},"source":["Load Kaggle Features"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"014D3-VYEeK1","colab":{}},"source":["GKaggle_Features=np.genfromtxt(path+\"XKaggleFeatures.csv\", delimiter=',')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kEnjr-7VExmd"},"source":["Split into Training and Testing"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8R1wcr_JE0L3","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(G_Features, Y, test_size=0.4, random_state=0)\n","print(X_train.shape)\n","print(X_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"G-O3PXMlFKX7"},"source":["# Random Forest Optimizing hyperparameters"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5SrAN8cs35IC","colab":{}},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import make_classification"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8QiifepeFNPW","colab":{}},"source":["n_estimators = [1, 2, 4, 8, 16, 32, 64]\n","train_results = []\n","test_results = []\n","for estimator in n_estimators:\n","    rf = RandomForestClassifier(n_estimators=estimator, n_jobs=-1)\n","    rf.fit(X_train, y_train)\n","    train_pred = rf.predict(X_train)\n","    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n","    roc_auc = auc(false_positive_rate, true_positive_rate)\n","    train_results.append(roc_auc)\n","    y_pred = rf.predict(X_test)\n","    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n","    roc_auc = auc(false_positive_rate, true_positive_rate)\n","    test_results.append(roc_auc)\n","from matplotlib.legend_handler import HandlerLine2D\n","line1, = plt.plot(n_estimators, train_results, 'b', label='Train AUC')\n","line2, = plt.plot(n_estimators, test_results, 'r', label='Test AUC')\n","plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n","plt.ylabel('AUC score')\n","plt.xlabel('n_estimators')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"11U9L9kHFn9Q","colab":{}},"source":["max_depths = np.linspace(1, 32, 32, endpoint=True)\n","train_results = []\n","test_results = []\n","for max_depth in max_depths:\n","    rf = RandomForestClassifier(max_depth=max_depth, n_jobs=-1)\n","    rf.fit(X_train, y_train)\n","    train_pred = rf.predict(X_train)\n","    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n","    roc_auc = auc(false_positive_rate, true_positive_rate)\n","    train_results.append(roc_auc)\n","    y_pred = rf.predict(X_test)\n","    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n","    roc_auc = auc(false_positive_rate, true_positive_rate)\n","    test_results.append(roc_auc)\n","from matplotlib.legend_handler import HandlerLine2D\n","line1, = plt.plot(max_depths, train_results, 'b', label='Train AUC')\n","line2, = plt.plot(max_depths, test_results, 'r', label='Test AUC')\n","plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n","plt.ylabel('AUC score')\n","plt.xlabel('Tree depth')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k2k1O02uGMYv"},"source":["# Random Forest Model and Prediction"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xxshp0_kGK31","colab":{}},"source":["clf = RandomForestClassifier(max_depth=9, n_estimators=15, random_state=0, )\n","clf.fit(X_train, y_train)\n","\n","y_pred=clf.predict(X_test)\n","FeatRandomAcc=(accuracy_score(y_test, y_pred))\n","FeatRandomF1=(f1_score(y_test, y_pred, pos_label=0))\n","print(FeatRandomAcc)\n","print(FeatRandomF1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"h9-UpsLswvHI","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tSYLZ70C42JT"},"source":["# Feature Scaling"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A6pbEvD35Dri","colab":{}},"source":["from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","scaler.fit(G_Features)\n","Xs=scaler.transform(G_Features)\n","X_trains, X_tests, y_trains, y_tests = train_test_split(Xs, Y, test_size=0.4, random_state=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"477XhlDh8XBF","colab":{}},"source":["scaler = MinMaxScaler()\n","scaler.fit(GKaggle_Features)\n","X_KaggleS=scaler.transform(GKaggle_Features)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1Fmm1CQ9JZBn"},"source":["# Multilayer Perceptron"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NSQ_3RvT6vYg"},"source":["getting correct parameters"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4A_6sd416sp-","colab":{}},"source":["clfmlp = MLPClassifier(max_iter=150)\n","parameter_space = {\n","    'hidden_layer_sizes': [(7,7,5), (7,5)],\n","    'activation': ['tanh', 'relu'],\n","}\n","from sklearn.model_selection import GridSearchCV\n","\n","clf = GridSearchCV(clfmlp, parameter_space, n_jobs=-1, scoring='f1' , cv=5, verbose=True)\n","clf.fit(X_trains, y_trains)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ixgASB9uDC5u","colab":{}},"source":["# Best paramete set\n","print('Best parameters found:\\n', clf.best_params_)\n","\n","# All results\n","means = clf.cv_results_['mean_test_score']\n","stds = clf.cv_results_['std_test_score']\n","for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n","    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D7bTAt9pJcPf","colab":{}},"source":["clfmlp = MLPClassifier(hidden_layer_sizes=(7,7,5), activation='tanh' ,max_iter=150 , random_state=3)\n","clfmlp.fit(X_trains,y_trains)\n","y_pred=clfmlp.predict(X_tests)\n","FeatMLPAcc=(accuracy_score(y_tests, y_pred))\n","FeatMLPF1=(f1_score(y_tests, y_pred, pos_label=0))\n","print(FeatMLPAcc)\n","print(FeatMLPF1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PHPekwOE7yXn","colab":{}},"source":["y_kaggle=clfmlp.predict(X_KaggleS)\n","\n","y_kaggle = zip(range(len(y_kaggle)), y_kaggle)\n","# Write the output in the format required by Kaggle\n","with open(path+\"predictionsnodesimMLPScaled150775.csv\",\"w\") as pred:\n","    csv_out = csv.writer(pred)\n","    csv_out.writerow(['id','predicted'])\n","    for row in y_kaggle:\n","        csv_out.writerow(row) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9SULQXWG6ZWE"},"source":["# Logistic Regression"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jtXY9Xnb59f9","colab":{}},"source":["from sklearn.linear_model import LogisticRegression \n","Logclassifier = LogisticRegression(random_state = 0) \n","Logclassifier.fit(X_trains, y_trains) \n","y_predLog = Logclassifier.predict(X_tests) \n","FeatLogAcc=(accuracy_score(y_tests, y_predLog))\n","FeatLogF1=(f1_score(y_tests, y_predLog, pos_label=0))\n","print(FeatLogAcc)\n","print(FeatLogF1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JFo7dHSgFrQ1","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zhM2wiqpGNE4"},"source":["# HADAMARD"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vY1gmIVZIqv4","colab":{}},"source":["model = Word2Vec.load(path+\"embeddingModeldim64Di.model\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q0gocWemGWki","colab":{}},"source":["Hadamard=[]\n","Hadamard_Kaggle=[]\n","empty=[]\n","for NodePair in X:\n","    try:\n","        Hadamard.append(model.wv.get_vector(NodePair[0])*model.wv.get_vector(NodePair[1]))\n","    except:\n","        empty.append[NodePair]\n","\n","for NodePair in X_Kaggle:\n","    try:\n","        Hadamard_Kaggle.append(model.wv.get_vector(NodePair[0])*model.wv.get_vector(NodePair[1]))\n","    except:\n","        empty.append[NodePair]\n","            \n","Hadamard=np.array(Hadamard)\n","Hadamard_Kaggle=np.array(Hadamard_Kaggle)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hSDF5mYgIais","colab":{}},"source":["X_trainH, X_testH, y_trainH, y_testH = train_test_split(Hadamard, Y, test_size=0.4, random_state=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TOOMdX3HJ0Ao"},"source":["### Logistic HADAMARD\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RW_6St0MJv7z","colab":{}},"source":["from sklearn.linear_model import LogisticRegression \n","Logclassifier = LogisticRegression(random_state = 0) \n","Logclassifier.fit(X_trainH, y_trainH) \n","y_predLog = Logclassifier.predict(X_testH) \n","HlogAcc=accuracy_score(y_testH, y_predLog)\n","HlogF1=f1_score(y_testH, y_predLog, pos_label=0)\n","print(HlogAcc)\n","print(HlogF1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XFmy_dNTKSC5"},"source":["### MLP HADAMARD"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OEWgCoThJ8I_","colab":{}},"source":["clfmlp = MLPClassifier(hidden_layer_sizes=(50,64), max_iter=100 , random_state=1)\n","clfmlp.fit(X_trainH,y_trainH)\n","y_pred=clfmlp.predict(X_testH)\n","HMLPAcc=accuracy_score(y_testH, y_pred)\n","HMLPF1=f1_score(y_testH, y_pred, pos_label=0)\n","print(HMLPAcc)\n","print(HMLPF1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"M8T1uopeLDy6"},"source":["### Random Forest HADAMARD"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"a-imnACHYDxG","colab":{}},"source":["clfRandom = RandomForestClassifier()\n","parameter_space = {\n","    'max_depth': [5, 10, 20, 30],\n","    'n_estimators': [5, 10, 15],\n","}\n","from sklearn.model_selection import GridSearchCV\n","\n","clf = GridSearchCV(clfRandom, parameter_space, n_jobs=-1, cv=3, verbose=True)\n","clf.fit(X_trainH, y_trainH)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zabBHzZyYrqb","colab":{}},"source":["# Best parameter set\n","print('Best parameters found:\\n', clf.best_params_)\n","\n","# All results\n","means = clf.cv_results_['mean_test_score']\n","stds = clf.cv_results_['std_test_score']\n","for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n","    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"18rTBIJBK0kp","colab":{}},"source":["clf = RandomForestClassifier(max_depth=20, n_estimators=15, random_state=0, )\n","clf.fit(X_trainH, y_trainH)\n","\n","y_pred=clf.predict(X_testH)\n","HrandomAcc=accuracy_score(y_testH, y_pred)\n","HrandomF1=f1_score(y_testH, y_pred, pos_label=0)\n","print(HrandomF1)\n","print(HrandomAcc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8DEkHq6eR1LG"},"source":["# Comparaisons "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H_B-l-EBLuIY","colab":{}},"source":["# set width of bar\n","barWidth = 0.25\n"," \n","# set height of bar\n","bars1 = [HlogAcc, HlogF1]\n","bars2 = [HMLPAcc, HMLPF1]\n","bars3 = [HrandomAcc, HrandomF1]\n"," \n","# Set position of bar on X axis\n","r1 = np.arange(len(bars1))\n","r2 = [x + barWidth for x in r1]\n","r3 = [x + barWidth for x in r2]\n"," \n","# Make the plot\n","plt.bar(r1, bars1, color='#7f6d5f', width=barWidth, edgecolor='white', label='LogisticR')\n","plt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='MLP')\n","plt.bar(r3, bars3, color='#2d7f5e', width=barWidth, edgecolor='white', label='RandomF')\n"," \n","# Add xticks on the middle of the group bars\n","plt.xlabel('Hadamard', fontweight='bold')\n","plt.xticks([r + barWidth for r in range(len(bars1))], ['Acc', 'F1'])\n","plt.ylim(0, 1)\n","# Create legend & Show graphic\n","plt.legend()\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yDxqBJytbKMA","colab":{}},"source":["# set width of bar\n","barWidth = 0.25\n"," \n","# set height of bar\n","bars1 = [FeatLogAcc, FeatLogF1]\n","bars2 = [FeatMLPAcc, FeatMLPF1]\n","bars3 = [FeatRandomAcc, FeatRandomF1]\n"," \n","# Set position of bar on X axis\n","r1 = np.arange(len(bars1))\n","r2 = [x + barWidth for x in r1]\n","r3 = [x + barWidth for x in r2]\n"," \n","# Make the plot\n","plt.bar(r1, bars1, color='#7f6d5f', width=barWidth, edgecolor='white', label='LogisticR')\n","plt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='MLP')\n","plt.bar(r3, bars3, color='#2d7f5e', width=barWidth, edgecolor='white', label='RandomF')\n"," \n","# Add xticks on the middle of the group bars\n","plt.xlabel('extracted features', fontweight='bold')\n","plt.xticks([r + barWidth for r in range(len(bars1))], ['Acc', 'F1'])\n"," \n","plt.ylim(0, 1)\n","# Create legend & Show graphic\n","plt.legend()\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yAgFoFgicxLA","colab":{}},"source":["from sklearn.model_selection import validation_curve\n","train_scores, valid_scores = validation_curve(RandomForestClassifier(), G_Features, Y, param_name=\"max_depth\", param_range=[4,6,8,10,15] ,scoring='f1', cv=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"u_PhSTNT2uMS","colab":{}},"source":["train_scores_mean = np.mean(train_scores, axis=1)\n","train_scores_std = np.std(train_scores, axis=1)\n","test_scores_mean = np.mean(valid_scores, axis=1)\n","test_scores_std = np.std(valid_scores, axis=1)\n","plt.plot([4,6,8,10,15], train_scores_mean, label=\"Training score\",\n","             color=\"darkorange\")\n","plt.plot([4,6,8,10,15], test_scores_mean, label=\"validation score\",\n","             color=\"navy\")\n","plt.legend()\n","plt.xlabel('Max depth')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PuIkxHRj7Dh1"},"source":["### Validation MLP"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"N2BiZTjL7Fty","colab":{}},"source":["from sklearn.model_selection import validation_curve\n","train_scores, valid_scores = validation_curve(MLPClassifier(hidden_layer_sizes=(7,5)), Xs, Y, param_name=\"max_iter\", param_range=[50,100,150,200] ,scoring='f1', cv=3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"K4kqK_rM9k5d","colab":{}},"source":["train_scores_mean = np.mean(train_scores, axis=1)\n","train_scores_std = np.std(train_scores, axis=1)\n","test_scores_mean = np.mean(valid_scores, axis=1)\n","test_scores_std = np.std(valid_scores, axis=1)\n","plt.plot([50,100,150,200] , train_scores_mean, label=\"Training score\",\n","             color=\"darkorange\")\n","plt.plot([50,100,150,200] , test_scores_mean, label=\"validation score\",\n","             color=\"navy\")\n","plt.legend()\n","plt.xlabel('Max iterations')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Bad_z95xBwlL","colab":{}},"source":["from sklearn.model_selection import validation_curve\n","train_scores, valid_scores = validation_curve(LogisticRegression(random_state = 0) , Xs, Y, param_name=\"C\", param_range=[0.005,0.05,0.5,1, 1.5] ,scoring='f1', cv=3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"djcai9I6xiWC","colab_type":"code","colab":{}},"source":["train_scores_mean = np.mean(train_scores, axis=1)\n","train_scores_std = np.std(train_scores, axis=1)\n","test_scores_mean = np.mean(valid_scores, axis=1)\n","test_scores_std = np.std(valid_scores, axis=1)\n","plt.plot([0.005,0.05,0.5,1, 1.5] , train_scores_mean, label=\"Training score\",\n","             color=\"darkorange\")\n","plt.plot([0.005,0.05,0.5,1, 1.5] , test_scores_mean, label=\"validation score\",\n","             color=\"navy\")\n","plt.legend()\n","plt.xlabel('Inverse of regularization')"],"execution_count":0,"outputs":[]}]}